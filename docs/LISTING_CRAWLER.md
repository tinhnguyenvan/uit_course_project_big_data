# Demo Tiki Listing Crawler vá»›i Resume

## ðŸŽ¯ TÃ­nh nÄƒng

1. âœ… **Crawl tá»« API listing**: `https://tiki.vn/api/personalish/v1/blocks/listings`
2. âœ… **PhÃ¢n trang tá»± Ä‘á»™ng**: Limit 10 items/page
3. âœ… **Resume crawling**: LÆ°u tráº¡ng thÃ¡i vÃ  tiáº¿p tá»¥c tá»« page Ä‘Ã£ crawl
4. âœ… **Push Kafka**: Má»—i item Ä‘Æ°á»£c push vÃ o Kafka topic
5. âœ… **Logging**: LÆ°u log crawl vÃ o database

## ðŸš€ Sá»­ dá»¥ng

### 1. Crawl vá»›i limit pages

```bash
# Crawl 10 pages Ä‘áº§u (category 870 - SÃ¡ch ká»¹ nÄƒng sá»‘ng)
SERVICE=crawl-listing CATEGORY_ID=870 MAX_PAGES=10 docker-compose up app

# Hoáº·c sá»­ dá»¥ng CLI
python src/manage.py crawl-listing --category-id 870 --max-pages 10
```

### 2. Crawl vá»›i resume (tiáº¿p tá»¥c tá»« page Ä‘Ã£ dá»«ng)

```bash
# Láº§n 1: Crawl 10 pages
SERVICE=crawl-listing CATEGORY_ID=870 MAX_PAGES=10 docker-compose up app

# Stop crawler (Ctrl+C)

# Láº§n 2: Tiáº¿p tá»¥c tá»« page 11
SERVICE=crawl-listing CATEGORY_ID=870 MAX_PAGES=10 docker-compose up app
# => Sáº½ crawl tá»« page 11-20
```

### 3. Disable resume (báº¯t Ä‘áº§u tá»« page 1)

```bash
SERVICE=crawl-listing CATEGORY_ID=870 MAX_PAGES=10 RESUME=false docker-compose up app

# Hoáº·c CLI
python src/manage.py crawl-listing --category-id 870 --no-resume
```

### 4. Crawl unlimited (Ä‘áº¿n khi háº¿t)

```bash
# KhÃ´ng set MAX_PAGES = crawl táº¥t cáº£
SERVICE=crawl-listing CATEGORY_ID=870 docker-compose up app
```

## ðŸ“Š Workflow Ä‘áº§y Ä‘á»§

### Terminal 1: Start Consumers
```bash
# Start consumers Ä‘á»ƒ nháº­n data tá»« Kafka
SERVICE=consumers-all docker-compose up -d app

# View logs
docker-compose logs -f app
```

### Terminal 2: Run Crawler
```bash
# Crawl 50 pages
SERVICE=crawl-listing CATEGORY_ID=870 MAX_PAGES=50 docker-compose up app
```

### Terminal 3: Monitor
```bash
# Check database
docker exec -it uit-bd-postgres psql -U uit_user -d uit_analytics

# Query products
SELECT COUNT(*) FROM products;

# Query crawl logs
SELECT * FROM crawl_logs ORDER BY started_at DESC LIMIT 5;

# Check last page crawled
SELECT error_message FROM crawl_logs 
WHERE crawler_type LIKE 'tiki_listing%' 
ORDER BY started_at DESC LIMIT 1;
```

## ðŸ” API Response Structure

API tráº£ vá» format nhÆ° trong `list.json`:

```json
{
  "data": [
    {
      "id": 278997895,
      "name": "SÃ¡ch Cá» Äá» Cá» Xanh",
      "price": 172500,
      "original_price": 230000,
      "discount_rate": 25,
      "rating_average": 0,
      "quantity_sold": {"value": 18},
      "thumbnail_url": "https://...",
      "seller_id": 1,
      "primary_category_path": "1/2/8322/316/870/67945/67946"
    }
  ],
  "paging": {
    "current_page": 1,
    "total": 2000,
    "last_page": 1000,
    "per_page": 2
  }
}
```

## ðŸ“ Resume Logic

Spider lÆ°u tráº¡ng thÃ¡i trong báº£ng `crawl_logs`:

```sql
-- Xem crawl logs
SELECT 
    log_id,
    crawler_type,
    status,
    items_crawled,
    error_message,  -- Chá»©a "Last page: 10"
    started_at,
    completed_at
FROM crawl_logs
WHERE crawler_type = 'tiki_listing_cat_870'
ORDER BY started_at DESC;
```

**Resume flow:**
1. Query `crawl_logs` Ä‘á»ƒ láº¥y last page
2. Parse tá»« `error_message`: `"Last page: 10"`
3. Tiáº¿p tá»¥c tá»« page `11`

## ðŸŽ² Test Resume

```bash
# Step 1: Crawl 5 pages
SERVICE=crawl-listing CATEGORY_ID=870 MAX_PAGES=5 docker-compose up app
# => Crawl pages 1-5

# Step 2: Crawl tiáº¿p 5 pages
SERVICE=crawl-listing CATEGORY_ID=870 MAX_PAGES=5 docker-compose up app
# => Crawl pages 6-10

# Step 3: Reset vÃ  crawl láº¡i tá»« Ä‘áº§u
SERVICE=crawl-listing CATEGORY_ID=870 MAX_PAGES=5 RESUME=false docker-compose up app
# => Crawl pages 1-5 láº¡i
```

## ðŸ—‚ï¸ Category IDs

**SÃ¡ch:**
- 870: SÃ¡ch ká»¹ nÄƒng sá»‘ng
- 871: SÃ¡ch tÆ° duy
- 316: SÃ¡ch thiáº¿u nhi
- 8322: SÃ¡ch tiáº¿ng Viá»‡t

**Äiá»‡n tá»­:**
- 1789: Äiá»‡n thoáº¡i
- 1520: Laptop
- 1846: MÃ¡y áº£nh

## ðŸ“ˆ Monitoring Kafka

```bash
# List topics
docker exec -it uit-bd-kafka kafka-topics --bootstrap-server localhost:9092 --list

# View messages
docker exec -it uit-bd-kafka kafka-console-consumer \
    --bootstrap-server localhost:9092 \
    --topic uit-products \
    --from-beginning \
    --max-messages 10

# Conduktor UI
open http://localhost:8081
```

## âš™ï¸ Configuration

Edit trong spider:
- `items_per_page = 10` - Items per page
- `DOWNLOAD_DELAY = 2` - Delay giá»¯a cÃ¡c requests
- `CONCURRENT_REQUESTS = 4` - Sá»‘ requests Ä‘á»“ng thá»i

## ðŸ› Debug

```bash
# View Scrapy logs
docker-compose logs app | grep "tiki_listing"

# Check last crawled page
python src/manage.py shell
>>> from app.models import SessionLocal, CrawlLog
>>> db = SessionLocal()
>>> log = db.query(CrawlLog).filter(
...     CrawlLog.crawler_type == 'tiki_listing_cat_870'
... ).order_by(CrawlLog.started_at.desc()).first()
>>> print(log.error_message)
Last page: 10
```
