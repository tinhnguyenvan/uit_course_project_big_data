# ğŸš€ Quick Start - Running Crawler

## âœ… Setup Ä‘Ã£ hoÃ n táº¥t

**Consumers Ä‘Ã£ cháº¡y background** - sáºµn sÃ ng nháº­n data tá»« Kafka

## ğŸ•·ï¸ Cháº¡y Crawler

### Option 1: Script tá»± Ä‘á»™ng (Khuyáº¿n nghá»‹)

```bash
# Cháº¡y crawler vá»›i resume (unlimited pages)
./run_crawler.sh

# Hoáº·c chá»‰ Ä‘á»‹nh category vÃ  limit pages
CATEGORY_ID=870 MAX_PAGES=50 ./run_crawler.sh

# Disable resume (báº¯t Ä‘áº§u tá»« page 1)
RESUME=false ./run_crawler.sh
```

### Option 2: Docker Compose trá»±c tiáº¿p

```bash
# Crawl unlimited pages (Ä‘áº¿n khi háº¿t)
SERVICE=crawl-listing CATEGORY_ID=870 docker-compose up app

# Crawl 100 pages
SERVICE=crawl-listing CATEGORY_ID=870 MAX_PAGES=100 docker-compose up app

# Disable resume
SERVICE=crawl-listing CATEGORY_ID=870 RESUME=false docker-compose up app
```

## ğŸ“Š Monitoring

### Terminal 1: Crawler logs
```bash
# Xem realtime logs cá»§a crawler
docker-compose logs -f app
```

### Terminal 2: Check database
```bash
# Connect PostgreSQL
docker exec -it uit-bd-postgres psql -U uit_user -d uit_analytics

# Queries
SELECT COUNT(*) FROM products;
SELECT COUNT(*) FROM shops;
SELECT * FROM crawl_logs ORDER BY started_at DESC LIMIT 5;

# Check last crawled page
SELECT 
    crawler_type,
    items_crawled,
    error_message,
    status,
    started_at
FROM crawl_logs 
WHERE crawler_type LIKE 'tiki_listing%' 
ORDER BY started_at DESC 
LIMIT 3;
```

### Terminal 3: Kafka monitoring
```bash
# Count messages
docker exec -it uit-bd-kafka kafka-run-class kafka.tools.GetOffsetShell \
    --broker-list localhost:9092 \
    --topic uit-products

# View messages
docker exec -it uit-bd-kafka kafka-console-consumer \
    --bootstrap-server localhost:9092 \
    --topic uit-products \
    --from-beginning \
    --max-messages 5
```

## ğŸ® Demo Resume Feature

```bash
# Step 1: Crawl 10 pages
CATEGORY_ID=870 MAX_PAGES=10 ./run_crawler.sh
# => Crawls pages 1-10

# Press Ctrl+C to stop

# Step 2: Resume crawling (next 10 pages)
CATEGORY_ID=870 MAX_PAGES=10 ./run_crawler.sh
# => Automatically continues from page 11-20

# Step 3: Resume again
CATEGORY_ID=870 MAX_PAGES=10 ./run_crawler.sh
# => Continues from page 21-30
```

## ğŸ”¥ Cháº¡y crawler liÃªn tá»¥c (Production Mode)

```bash
# Crawl táº¥t cáº£ products trong category (cÃ³ thá»ƒ máº¥t nhiá»u giá»)
CATEGORY_ID=870 ./run_crawler.sh

# Crawler sáº½:
# âœ… Tá»± Ä‘á»™ng phÃ¢n trang
# âœ… Push tá»«ng item vÃ o Kafka
# âœ… LÆ°u progress vÃ o database
# âœ… Auto-resume náº¿u restart
```

## ğŸ›‘ Stop Crawler

```bash
# Press Ctrl+C trong terminal Ä‘ang cháº¡y crawler

# Hoáº·c force stop
docker-compose stop app
```

## ğŸ“ˆ Web UIs

- **Conduktor Console** (Kafka): http://localhost:8081
- **Metabase** (Analytics): http://localhost:3000

## ğŸ“ Logs

```bash
# View all logs
docker-compose logs

# Only crawler logs
docker-compose logs app

# Follow logs
docker-compose logs -f app

# Last 100 lines
docker-compose logs --tail 100 app
```

## âš™ï¸ Configuration

Edit trong [docker-compose.yml](../docker-compose.yml):

```yaml
environment:
  CATEGORY_ID: ${CATEGORY_ID:-870}    # Default category
  MAX_PAGES: ${MAX_PAGES:-}           # Unlimited by default
  RESUME: ${RESUME:-true}             # Resume enabled
```

## ğŸ› Troubleshooting

**Crawler khÃ´ng cháº¡y:**
```bash
# Check app status
docker-compose ps app

# Restart app
docker-compose restart app
```

**Consumer khÃ´ng nháº­n data:**
```bash
# Check consumers
SERVICE=consumers-all docker-compose up -d app
docker-compose logs -f app
```

**Database connection error:**
```bash
# Check PostgreSQL
docker-compose ps postgres
docker-compose logs postgres
```

## ğŸ¯ Current Status

âœ… **Consumers running** - Background, listening for messages
ğŸ”„ **Ready to crawl** - Run `./run_crawler.sh` to start

## ğŸ“Š Expected Results

Vá»›i category 870 (SÃ¡ch ká»¹ nÄƒng sá»‘ng):
- Total pages: ~1000+ pages
- Items per page: 10
- Total products: ~10,000+
- Crawl time: ~5-10 hours (vá»›i delay 2s/request)
