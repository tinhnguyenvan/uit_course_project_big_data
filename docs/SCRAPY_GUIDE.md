# UIT Big Data Project - Scrapy Web Crawling

## ğŸ“‹ Cáº¥u trÃºc Project

```
src/app/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ config.py                    # Configuration settings
â”œâ”€â”€ manage.py                    # CLI management tool
â”‚
â”œâ”€â”€ models/                      # SQLAlchemy ORM Models
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ database.py             # Database connection
â”‚   â””â”€â”€ models.py               # Product, Review, Shop, etc.
â”‚
â”œâ”€â”€ schemas/                     # Pydantic Schemas
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ products.py
â”‚   â”œâ”€â”€ reviews.py
â”‚   â”œâ”€â”€ shops.py
â”‚   â”œâ”€â”€ categories.py
â”‚   â””â”€â”€ prices.py
â”‚
â”œâ”€â”€ crawlers/                    # Scrapy Project
â”‚   â”œâ”€â”€ settings.py             # Scrapy settings
â”‚   â”œâ”€â”€ items.py                # Scrapy items
â”‚   â”œâ”€â”€ middlewares.py          # Scrapy middlewares
â”‚   â”œâ”€â”€ pipelines.py            # Item pipelines (Kafka producer)
â”‚   â””â”€â”€ spiders/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ tiki_products.py    # Product spider
â”‚       â””â”€â”€ tiki_reviews.py     # Review spider
â”‚
â”œâ”€â”€ consumers/                   # Kafka Consumers
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_consumer.py        # Base consumer class
â”‚   â”œâ”€â”€ product_consumer.py     # Product consumer
â”‚   â”œâ”€â”€ review_consumer.py      # Review consumer
â”‚   â””â”€â”€ run_all.py              # Run all consumers
â”‚
â””â”€â”€ utils/                       # Utilities
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ logger.py               # Logging setup
    â”œâ”€â”€ kafka_utils.py          # Kafka helpers
    â””â”€â”€ db_utils.py             # Database helpers
```

## ğŸš€ CÃ i Ä‘áº·t

### 1. Clone vÃ  cÃ i Ä‘áº·t dependencies

```bash
cd /Users/tinhnguyen/Sites/UIT/12_cong_nghe_du_lieu_lon/project

# Táº¡o virtual environment
python -m venv venv
source venv/bin/activate  # macOS/Linux

# CÃ i Ä‘áº·t packages
pip install -r requirements.txt
```

### 2. Táº¡o file .env

```bash
cp .env.example .env
# Edit .env náº¿u cáº§n thay Ä‘á»•i cáº¥u hÃ¬nh
```

### 3. Khá»Ÿi Ä‘á»™ng Docker services

```bash
docker-compose up -d
```

### 4. Kiá»ƒm tra káº¿t ná»‘i

```bash
# Sá»­ dá»¥ng CLI tool
python src/manage.py check
```

Output:
```
ğŸ” Checking system connections...

ğŸ“Š PostgreSQL:
âœ“ Database connection successful

Table statistics:
  - products: 0 rows
  - reviews: 0 rows
  - shops: 0 rows
  ...

ğŸ“¨ Kafka:
âœ“ Kafka connection successful
```

### 5. Khá»Ÿi táº¡o database

```bash
python src/manage.py init-db
```

### 6. Táº¡o Kafka topics

```bash
python src/manage.py create-kafka-topics
```

## ğŸ“ Sá»­ dá»¥ng

### 1. Crawl Products

```bash
# Crawl 10 pages tá»« category Electronics (1789)
python src/manage.py crawl-products --category-id 1789 --max-pages 10

# Hoáº·c sá»­ dá»¥ng Scrapy trá»±c tiáº¿p
cd src
scrapy crawl tiki_products -a category_id=1789 -a max_pages=10
```

### 2. Crawl Reviews

```bash
# Crawl reviews cho products (comma-separated IDs)
python src/manage.py crawl-reviews --product-ids "123456,789012" --max-pages 5

# Hoáº·c sá»­ dá»¥ng Scrapy trá»±c tiáº¿p
cd src
scrapy crawl tiki_reviews -a product_ids="123456,789012" -a max_pages=5
```

### 3. Cháº¡y Kafka Consumers

```bash
# Cháº¡y táº¥t cáº£ consumers
python src/manage.py start-consumers --consumer all

# Hoáº·c cháº¡y tá»«ng consumer riÃªng
python src/manage.py start-consumers --consumer products
python src/manage.py start-consumers --consumer reviews
```

### 4. Xem thá»‘ng kÃª

```bash
python src/manage.py stats
```

## ğŸ”„ Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Scrapy Spider  â”‚
â”‚  (tiki_products)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kafka Producer  â”‚â”€â”€â”€â”€â”€â–¶â”‚ Kafka Topic  â”‚
â”‚   (Pipeline)    â”‚      â”‚uit-products  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚   Consumer   â”‚
                         â”‚  (products)  â”‚
                         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚  PostgreSQL  â”‚
                         â”‚ (TimescaleDB)â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Kafka Topics

| Topic | Partitions | Description |
|-------|-----------|-------------|
| `uit-products` | 3 | Product data |
| `uit-reviews` | 3 | Review data |
| `uit-prices` | 3 | Price history |
| `uit-shops` | 1 | Shop/seller data |

## ğŸ¯ Spider Options

### Product Spider

```bash
scrapy crawl tiki_products \
    -a category_id=1789 \      # Category ID (default: 1789 - Electronics)
    -a max_pages=10            # Max pages to crawl (default: 10)
```

**Tiki Category IDs:**
- 1789: Äiá»‡n thoáº¡i - MÃ¡y tÃ­nh báº£ng
- 1520: Laptop
- 1846: MÃ¡y áº£nh
- 1882: Äá»“ng há»“ thÃ´ng minh
- 27498: Tivi
- 1801: Tai nghe

### Review Spider

```bash
scrapy crawl tiki_reviews \
    -a product_ids="12345,67890" \  # Comma-separated product IDs
    -a max_pages=5                   # Max pages per product (default: 5)
```

## ğŸ› ï¸ CLI Commands

```bash
# Check system
python src/manage.py check

# Initialize database
python src/manage.py init-db
python src/manage.py init-db --reset  # Drop and recreate

# Create Kafka topics
python src/manage.py create-kafka-topics

# Crawl data
python src/manage.py crawl-products --category-id 1789 --max-pages 10
python src/manage.py crawl-reviews --product-ids "123,456" --max-pages 5

# Start consumers
python src/manage.py start-consumers --consumer all
python src/manage.py start-consumers --consumer products
python src/manage.py start-consumers --consumer reviews

# View statistics
python src/manage.py stats
```

## ğŸ“¦ Database Schema

### Products
- product_id (PK)
- name, description, url, image_url
- shop_id (FK), category_id (FK)
- rating, sold_count
- first_seen, last_updated

### Reviews
- review_id (PK)
- product_id (FK)
- user_name, rating, comment
- has_images, helpful_count
- created_at, crawled_at

### Product Prices (TimescaleDB Hypertable)
- product_id (PK)
- price, original_price, discount_percent
- stock_available
- timestamp (PK)

### Shops
- shop_id (PK)
- shop_name, rating, response_rate
- follower_count, is_official
- created_at, last_updated

## ğŸ” Monitoring

### Conduktor Console
- URL: http://localhost:8081
- View Kafka topics, messages, consumer groups

### Metabase
- URL: http://localhost:3000
- Create dashboards, visualizations

### PostgreSQL
```bash
docker exec -it uit-bd-postgres psql -U uit_user -d uit_analytics
```

## ğŸ› Debugging

### View Scrapy logs
```bash
tail -f logs/scrapy.log
```

### View consumer logs
```bash
tail -f logs/consumers.log
```

### Check Kafka messages
```bash
# List topics
docker exec -it uit-bd-kafka kafka-topics --bootstrap-server localhost:9092 --list

# Consume messages
docker exec -it uit-bd-kafka kafka-console-consumer \
    --bootstrap-server localhost:9092 \
    --topic uit-products \
    --from-beginning
```

## ğŸ“ Next Steps

1. **Implement Sentiment Analysis**
   - Add sentiment consumer
   - Use Vietnamese NLP (underthesea)
   - Save to review_sentiment table

2. **Add Apache Airflow**
   - Schedule crawlers
   - Monitor pipelines
   - Handle failures

3. **Create API Layer**
   - FastAPI for REST endpoints
   - Query products, reviews
   - Trigger crawls

4. **Build Dashboards**
   - Product rankings
   - Price trends
   - Sentiment analysis

## ğŸ“š Resources

- [Scrapy Documentation](https://docs.scrapy.org/)
- [Kafka Python](https://docs.confluent.io/kafka-clients/python/current/overview.html)
- [SQLAlchemy](https://docs.sqlalchemy.org/)
- [TimescaleDB](https://docs.timescale.com/)
