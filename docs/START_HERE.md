# ğŸ‰ Há»† THá»NG ÄÃƒ Sáº´N SÃ€NG!

## âœ… TÃ¬nh tráº¡ng hiá»‡n táº¡i

1. **Docker containers Ä‘ang cháº¡y:**
   - âœ… PostgreSQL (port 54325)
   - âœ… Kafka (port 9092)
   - âœ… Zookeeper (port 2181)
   - âœ… Metabase (port 3000)
   - âœ… Conduktor Console (port 8081)
   - âœ… **Kafka Consumers (cháº¡y background)**

2. **ÄÃ£ test thÃ nh cÃ´ng:**
   - âœ… Crawler hoáº¡t Ä‘á»™ng
   - âœ… Push data vÃ o Kafka
   - âœ… Consumers nháº­n vÃ  xá»­ lÃ½ data
   - âœ… **Resume crawling tá»« page Ä‘Ã£ dá»«ng**

## ğŸš€ CHáº Y CRAWLER NGAY

### Option 1: Cháº¡y vÃ´ thá»i háº¡n (Crawl táº¥t cáº£)
```bash
./run_crawler.sh
```
**Crawler sáº½ crawl Táº¤T Cáº¢ products cho Ä‘áº¿n khi háº¿t (~200 pages = 2000 products)**

### Option 2: Cháº¡y vá»›i limit
```bash
# Crawl 50 pages
CATEGORY_ID=870 MAX_PAGES=50 ./run_crawler.sh

# Crawl 100 pages
CATEGORY_ID=870 MAX_PAGES=100 ./run_crawler.sh
```

### Option 3: Docker compose trá»±c tiáº¿p
```bash
# Unlimited pages
SERVICE=crawl-listing CATEGORY_ID=870 docker-compose up app

# With limit
SERVICE=crawl-listing CATEGORY_ID=870 MAX_PAGES=50 docker-compose up app
```

## ğŸ“Š DEMO RESUME

ÄÃ¢y lÃ  Ä‘iá»ƒm máº¡nh cá»§a há»‡ thá»‘ng:

```bash
# Láº§n 1: Crawl 10 pages
CATEGORY_ID=870 MAX_PAGES=10 ./run_crawler.sh
# => Crawls pages 1-10
# Press Ctrl+C Ä‘á»ƒ dá»«ng

# Láº§n 2: Tá»° Äá»˜NG tiáº¿p tá»¥c tá»« page 11
CATEGORY_ID=870 MAX_PAGES=10 ./run_crawler.sh  
# => Crawls pages 11-20 (khÃ´ng pháº£i tá»« Ä‘áº§u!)

# Láº§n 3: Tiáº¿p tá»¥c
CATEGORY_ID=870 MAX_PAGES=10 ./run_crawler.sh
# => Crawls pages 21-30
```

## ğŸ“ˆ MONITOR

### Terminal 1: Crawler logs
```bash
docker-compose logs -f app
```

### Terminal 2: Check database realtime
```bash
# Connect
docker exec -it uit-bd-postgres psql -U uit_user -d uit_analytics

# Check products count
SELECT COUNT(*) FROM products;

# Check shops
SELECT COUNT(*) FROM shops;

# View latest products
SELECT product_id, name, price, sold_count 
FROM products 
ORDER BY first_seen DESC 
LIMIT 10;

# Check crawl progress
SELECT 
    crawler_type,
    items_crawled,
    error_message,  -- Contains "Last page: XX"
    started_at,
    status
FROM crawl_logs 
ORDER BY started_at DESC 
LIMIT 5;
```

### Terminal 3: Kafka messages
```bash
# Count messages in topic
docker exec -it uit-bd-kafka kafka-run-class kafka.tools.GetOffsetShell \
    --broker-list localhost:9092 \
    --topic uit-products

# View messages
docker exec -it uit-bd-kafka kafka-console-consumer \
    --bootstrap-server localhost:9092 \
    --topic uit-products \
    --from-beginning \
    --max-messages 10 | jq '.'
```

## ğŸ® WEB UIs

- **Conduktor Console** (Kafka Management): http://localhost:8081
- **Metabase** (Analytics & BI): http://localhost:3000

## ğŸ”„ WORKFLOW HOÃ€N CHá»ˆNH

```
Tiki API 
   â†“ (Scrapy Spider)
Kafka Topics (uit-products, uit-shops)
   â†“ (Kafka Consumers - Ä‘ang cháº¡y background)
PostgreSQL Database
   â†“ (Metabase)
Analytics & Visualization
```

## âš™ï¸ Cáº¤U HÃŒNH

Edit trong spider náº¿u cáº§n:
- **items_per_page**: 10 (items/page)
- **DOWNLOAD_DELAY**: 2 seconds (giá»¯a cÃ¡c requests)
- **CONCURRENT_REQUESTS**: 4 (requests Ä‘á»“ng thá»i)

## ğŸ›‘ STOP/RESTART

```bash
# Stop crawler (Ctrl+C trong terminal Ä‘ang cháº¡y)

# Stop consumers
docker-compose stop app

# Restart consumers
SERVICE=consumers-all docker-compose up -d app

# Stop everything
docker-compose down

# Restart everything
./start_system.sh
```

## ğŸ“ LOGS & DATA

```bash
# Scrapy logs
ls logs/

# Exported data (JSONL)
ls data/

# View log file
tail -f logs/scrapy.log
```

## ğŸ¯ EXPECTED RESULTS

**Category 870 (SÃ¡ch ká»¹ nÄƒng sá»‘ng):**
- Total pages: ~200
- Items per page: 10
- **Total products: ~2,000**
- Estimated time: 
  - 50 pages: ~3-5 minutes
  - 100 pages: ~6-10 minutes
  - 200 pages (full): ~12-20 minutes

**Vá»›i DOWNLOAD_DELAY=2s:**
- Requests: 200 pages = ~400 seconds = ~7 phÃºt
- + Processing time

## ğŸš¨ TROUBLESHOOTING

### Foreign Key Error (shops)

Hiá»‡n táº¡i tháº¥y lá»—i nÃ y khi insert products:
```
foreign key constraint "products_shop_id_fkey"
Key (shop_id)=(1) is not present in table "shops"
```

**Giáº£i phÃ¡p**: Consumers sáº½ tá»± Ä‘á»™ng insert shops trÆ°á»›c khi insert products. Hoáº·c disable foreign key constraint táº¡m thá»i.

### Check logs
```bash
docker-compose logs app | grep ERROR
docker-compose logs app | grep WARNING
```

## ğŸŠ Báº®T Äáº¦U NGAY!

```bash
# Simple - cháº¡y crawler vá»›i resume enabled
./run_crawler.sh
```

**Há»‡ thá»‘ng sáº½:**
1. âœ… Tá»± Ä‘á»™ng phÃ¢n trang (10 items/page)
2. âœ… Push tá»«ng item vÃ o Kafka  
3. âœ… Consumers xá»­ lÃ½ realtime
4. âœ… LÆ°u vÃ o PostgreSQL
5. âœ… LÆ°u progress Ä‘á»ƒ resume
6. âœ… Tá»± Ä‘á»™ng tiáº¿p tá»¥c khi restart

**Enjoy! ğŸ‰**
